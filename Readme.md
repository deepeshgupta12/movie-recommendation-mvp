# ğŸ¬ Movie Recommendation MVP (V2 + UI)

A **local-first, production-style Movie Recommendation System** built on the **MovieLens 20M** dataset.  
This repo is designed as a serious, stepwise MVP that mirrors **real-world recommender architecture** used by platforms like **Netflix** and **Amazon Prime Video**, but deliberately scoped to remain:

- âœ… **reproducible**
- âœ… **offline-evaluable**
- âœ… **explainable**
- âœ… **fast on Apple Silicon (M1)**
- âœ… **productized locally** with API + UI

If you want to demonstrate an end-to-end recommenderâ€”from data to a user-facing experienceâ€”without cloud infrastructure, this is the right blueprint.

---

## ğŸ§­ What This Project Is (and Isnâ€™t)

### âœ… This is
A complete **multi-stage recommender MVP** with:

- ğŸ“¥ Robust ingestion and analytics layer  
- ğŸ” Hybrid candidate retrieval  
- ğŸ§® Time-aware ranking  
- ğŸ§  Genre affinity cross-features  
- ğŸ—ï¸ Feature store  
- ğŸŒ FastAPI service  
- ğŸ–¥ï¸ Streamlit UI with posters + feedback loop  
- âš¡ Real-time UI-only personalization  

### âŒ This is not
- A full-scale deep learning recommender platform  
- A true online-learning system  
- A direct replica of Netflix/Prime infrastructure  
- A cloud-scale retrieval/ranking system with petabyte telemetry  

This is intentional. The goal is to be **production-faithful**, not production-identical.

---

## ğŸ§  Why This Approach Is Different

Most hobby or tutorial recommenders stop at:

- popularity  
- matrix factorization  
- basic item-item similarity  

They often lack:
- robust offline evaluation  
- time-based splitting  
- multi-source retrieval  
- ranking layers  
- explainability  
- product surfaces  

This MVP goes beyond that by adopting **modern multi-stage design**:

### ğŸ”¥ Key differentiators
1) **Hybrid retrieval, not single-model recommendations**  
   We generate candidates from multiple â€œexpertsâ€:
   - â­ Popularity  
   - ğŸ” Item-Item similarity  
   - ğŸ¤ ALS-based collaborative filtering  
   - ğŸ­ Genre neighbors  

   This is closer to how large platforms build resilient candidate sets.

2) **Time-aware scoring with decay signals**  
   Most public projects ignore recency dynamics.  
   This MVP explicitly models:
   - user recency  
   - item recency  
   - confidence decay  

3) **Feature store-first mindset**  
   We precompute:
   - user-level aggregates  
   - item-level aggregates  
   - genre priors and affinity  
   so the ranker and service work efficiently.

4) **Ranker with cross-feature logic**  
   Instead of naive blending or fixed rules, we train a ranker using:
   - user activity + confidence sums  
   - item popularity + confidence sums  
   - days since last interaction  
   - âœ… userâ€“item genre affinity cross-features  

5) **Explainability baked into service output**  
   Each recommendation can surface:
   - â€œTrending nowâ€  
   - â€œMatches your genresâ€  
   - â€œSimilar to your tasteâ€  
   - â€œBoosted by your likesâ€ (UI layer)

6) **Full product loop**
   Not just models:
   - API  
   - UI  
   - posters  
   - feedback logging  
   - real-time personalization  

---

## ğŸ“º How This Matches Netflix / Amazon Prime (Conceptually)

Netflix and Amazon Prime operate with:

- massive behavioral telemetry  
- deep personalization stacks  
- multi-model ensembles  
- sophisticated experimentation systems  
- large-scale ranking infrastructure  
- online learning and real-time feature pipelines  

We obviously wonâ€™t replicate that scale locally.  
But the **shape of the architecture** is similar.

### âœ… Where we match
- âœ… **Multi-stage architecture** (retrieve â†’ rank)  
- âœ… **Hybrid candidate sources**  
- âœ… **Recency-aware modeling**  
- âœ… **Feature store mindset**  
- âœ… **Explainability + product surfaces**  
- âœ… **Feedback-driven personalization (simulated)**  

### âš ï¸ Where we do not match (by design)
- âŒ No neural retrieval models (two-tower)  
- âŒ No real user session tracking  
- âŒ No real-time streaming pipelines (Kafka/Flink)  
- âŒ No A/B experimentation engine  
- âŒ No multi-device context modeling  
- âŒ No large-scale knowledge graph enrichment  
- âŒ No cloud-scale ANN infrastructure  

This repo is a **faithful MVP** of the *architecture pattern*, not the *enterprise system*.

---

## ğŸ§± System Architecture (MVP)

### 1) ğŸ“¥ Data Layer
- Downloads MovieLens 20M
- Builds Parquet datasets
- Creates DuckDB views for fast analytics

**Why it matters:**
- Produces a clean offline data foundation  
- Enables faster iteration  
- Mimics real-world warehouse â†’ feature store flows  

---

### 2) ğŸ” Candidate Generation (V1.5)

Candidate sources:
- â­ Popularity  
- ğŸ” Item-Item similarity  
- ğŸ¤ ALS CF (implicit confidence)  
- ğŸ­ Genre neighbors  

They are combined into a **hybrid retrieval pool**.

**Why it matters:**
- Single-model retrieval is fragile  
- Hybrid retrieval improves recall coverage  
- Mirrors real-world ensemble strategy  

---

### 3) ğŸ—ï¸ Feature Store (V2)

Generated files:
- ğŸ‘¤ `user_features.parquet`
- ğŸï¸ `item_features.parquet`
- ğŸ­ `genre_item_priors.parquet`
- ğŸ§© Expanded genre + affinity lookups

Feature themes:
- interaction volume  
- confidence sums  
- decay-weighted confidence  
- recency tracking  
- genre priors  

**Why it matters:**
- Separates compute-heavy aggregation from real-time ranking  
- Enables fast and stable inference  
- Reflects production best practice  

---

### 4) ğŸ§® Ranking (V1 â†’ V2)

Ranker:
- **HistGradientBoostingClassifier**

V2 features include locked ordering:
- user_interactions  
- user_conf_sum  
- user_conf_decay_sum  
- user_days_since_last  
- item_interactions  
- item_conf_sum  
- item_conf_decay_sum  
- item_days_since_last  

Plus:
- âœ… **genre-level cross intelligence**

**Why it matters:**
- Moves beyond heuristic blending  
- Improves top-K precision  
- Creates an interpretable, scalable local ranker  

---

### 5) ğŸŒ Serving Layer (Step 6)

- **V2RecommenderService**
- **FastAPI**
- Debug endpoints and reason tags

Endpoints:
- `/health`
- `/recommend/user/{user_idx}?k=`
- `/recommend/user/{user_idx}/debug?k=`

**Why it matters:**
- Converts a model into a product capability  
- Makes your MVP callable and testable  
- Enables UI integration  

---

### 6) ğŸ–¥ï¸ UI Layer (Step 7)

Built with **Streamlit** to avoid frontend overhead.

#### âœ… Step 7.1
- UI that calls FastAPI
- Displays ranked recommendations

#### âœ… Step 7.2
- ğŸï¸ Poster support via TMDB (optional)
- Local multi-threaded poster cache
- ğŸ‘/ğŸ‘ â€œLike/Dislike/Saveâ€ feedback stored locally

#### âœ… Step 7.3
- âš¡ Real-time UI-only re-ranking based on likes
- â€œYour recent tasteâ€ panel
- Cached API calls for smoother UX

**Why it matters:**
- Demonstrates a full user-facing loop  
- Enables realistic product demos  
- Simulates personalization before online learning  

---

## ğŸ“Š Offline Evaluation Philosophy

We use:
- **time-based splits**
- hybrid recall evaluation  
- ranked hybrid evaluation  

Metrics:
- Recall@10/20/50  
- NDCG@10/20/50  

**Why it matters:**
- Random splits often inflate results  
- Time-based evaluation is closer to real consumption  
- Helps prevent false confidence in model quality  

---

## ğŸ“ˆ Latest Offline Results Snapshot (V2 Ranked Hybrid)

Your recent successful run showed healthy top-K improvements, approximately:

- âœ… recall@10 â‰ˆ 0.0253  
- âœ… recall@20 â‰ˆ 0.0454  
- âœ… recall@50 â‰ˆ 0.0592  
- âœ… NDCG improved across top-K  

Interpretation:
- Candidate layer provides breadth  
- Ranker improves relevance at the top  

---

## ğŸ§° Tech Stack

- ğŸ Python 3.11
- âš¡ Polars
- ğŸ¦† DuckDB
- ğŸ¤ implicit
- ğŸŒ² scikit-learn
- ğŸ“ˆ tqdm
- ğŸŒ FastAPI + Uvicorn
- ğŸ§± Joblib
- ğŸ–¥ï¸ Streamlit
- ğŸŒ Requests

---

## ğŸ“ High-Level Project Layout

```text
movie-recommendation-mvp/
â”œâ”€â”€ app/                        # FastAPI app
â”œâ”€â”€ ui/                         # Streamlit UI + feedback + rerank
â”œâ”€â”€ scripts/                    # CLI runners / demos / eval flows
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/                 # settings
â”‚   â”œâ”€â”€ data/                   # download, ingest, duckdb
â”‚   â”œâ”€â”€ eval/                   # splits + metrics
â”‚   â”œâ”€â”€ models/                 # popularity, item-item, ALS
â”‚   â”œâ”€â”€ retrieval/              # hybrid blending + genre neighbors
â”‚   â”œâ”€â”€ ranking/                # training data + rankers + feature store
â”‚   â”œâ”€â”€ metadata/               # poster cache (TMDB optional)
â”‚   â””â”€â”€ service/                # V2 inference layer
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â””â”€â”€ reports/models/             # trained rankers + meta
```

---

## âš™ï¸ Setup

```bash
cd movie-recommendation-mvp
python -m venv .venv
source .venv/bin/activate
pip install -U pip
```

Install dependencies:

```bash
pip install polars duckdb implicit scikit-learn tqdm fastapi uvicorn joblib streamlit requests
```

---

## ğŸ§ª Step-wise Execution

### âœ… Step 1: Data Build

```bash
source .venv/bin/activate
python -m scripts.download_movielens
python -m src.data.ingest_movielens
python -m src.data.create_duckdb
```

---

### âœ… Step 2: V1 Baselines

```bash
python -m scripts.eval_v1_baselines
```

---

### âœ… Step 3: V1.5 Hybrid Candidates

```bash
python -m src.data.prepare_implicit_confidence
python -m src.eval.split_confidence
python -m scripts.eval_v1_5_candidates
```

---

### âœ… Step 4: Ranking (V1)

```bash
python -m src.ranking.build_training_data
python -m src.ranking.train_ranker
python -m scripts.eval_ranked_candidates
```

---

### âœ… Step 5: V2 MVP Upgrades

```bash
python -m src.ranking.feature_store
python -m scripts.eval_v2_candidates
python -m src.ranking.train_ranker_v2
python -m scripts.eval_ranked_candidates_v2
```

---

### âœ… Step 6: Serving

```bash
uvicorn app.main:app --reload
```

Test:

```bash
curl http://127.0.0.1:8000/health
curl "http://127.0.0.1:8000/recommend/user/9764?k=10"
curl "http://127.0.0.1:8000/recommend/user/9764/debug?k=5"
```

---

### âœ… Step 7: UI

Run UI:

```bash
python -m scripts.run_ui
```

---

## ğŸï¸ Optional Posters (TMDB)

Set key:

```bash
export TMDB_API_KEY="YOUR_KEY"
```

Build cache:

```bash
python -m scripts.build_posters_cache
```

Output:

- `data/processed/item_posters.json`

---

## ğŸ‘ Feedback Loop (Local)

UI logs feedback to:

- `data/processed/ui_feedback.jsonl`

Actions:
- ğŸ‘ Like
- ğŸ‘ Dislike
- â­ Save

---

## âš¡ Real-time Personalization (UI-only)

When enabled in UI:
- reads liked genres from feedback logs  
- applies a small boost to matching items  
- reorders the visible top-K  

This simulates an online personalization feel  
without retraining or streaming infra.

---

## ğŸ” Why This MVP Is Useful in the Real World

Even without deep learning:

- This repo is a strong template for:
  - enterprise prototyping  
  - internal stakeholder demos  
  - offline evaluation pipelines  
  - recommender learning projects  
  - local-first experimentation  

It teaches the *product shape* of recommender systems:
not just the models.

---

## ğŸ›£ï¸ Roadmap Beyond This MVP

Potential next expansions aligned with production-grade systems:

### ğŸ§  Retrieval upgrades
- Two-tower embedding retrieval  
- ANN with FAISS  

### ğŸ¯ Ranking upgrades
- Pairwise or listwise ranking  
- LightGBM ranking objectives  

### ğŸ” Session intelligence
- â€œBecause you watched Xâ€ logic  
- short-term intent modeling  
- time-of-day personalization  

### ğŸ§ª Experimentation
- offline compare harness  
- synthetic A/B simulation  
- evaluation dashboards  

---

## ğŸ™Œ Credits & Data

- Dataset: **MovieLens 20M** by GroupLens Research  
- Used here for education, research, and MVP prototyping.

---

## ğŸ“Œ Status

âœ… **V2 multi-stage recommender + API + UI + feedback loop complete**  
Next optional milestone: **Step 7.4** (session-style UI modules and â€œBecause you watchedâ€ experiences).
